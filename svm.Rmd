---
title: "Support Vector Machines"
output: html_notebook
---

# Introduction

Support vector machines are a widely used supervised learning model using for classification and regression. SVM attempts to fit a hyperplane which maximizes the distance separating samples from the decision boundary. By exploiting kernels, SVM can perform non-linear classification by implicitly mapping inputs into higher dimensions. 

## Binary classification
We want to learn a function that maps $x \in \mathbb{R}^d \rightarrow \{-1,-1\}$. Denote our dataset as $\mathcal{D} = \{x_i, y_i\}$, for $i \in [1,\dots,n]$.

Let's derive the equation of a plane. Assume we have some plane in some vector space. A point $P = \langle x,y,z \rangle$ lies on that plane. The normal to the plane is $\vec{w}$. Another point $P_0 = \langle x_0,y_0,z_0 \rangle$ lies on the plane. The vector from $P$ to $P_0$ is given by $\vec{P_0} - \vec{P}$ is equal to $\langle x_0-x, y_0-y, z_0-z \rangle$. Since this is orthogonal to $w$ (by definition): 
\begin{align*}
\vec{w} \cdot (\vec{P_0} - \vec{P}) &= 0 \\
\vec{w} \cdot \vec{P_0} - \vec{w} \cdot \vec{P} &= 0 \\
\vec{w} \cdot \vec{P}  + b &= 0
\end{align*}

Consider the vector space of our data points, $x_i$ for $i \in [1,n]$. We describe a separating hyperplane as $w^T x + w_0 = 0$. Here, $w$ is the normal vector of the plane. 

### Classifier
Let's define our classifier as follows: 
$$
h(x) = \begin{cases}
  1 \quad&\text{if } w^Tx + b \geq 0 \\
  -1 \quad&\text{if } w^Tx + b < 0
\end{cases}
$$
In other words, if the point lies above the hyperplane, it will be classified in the positive class. Otherwise, it will be classified in the negative class. The next step is to find the optimal hyperplane to separate the data. 

### Geometric Margin
Consider the following formulation: 
$$
\gamma = y (\frac{w}{||w||}\cdot x + \frac{b}{||w||})
$$
For a correctly classified positive sample that is far from the plane, both the $y$ and the term in parentheses will be positive, so $\gamma$ will be large and positive. For a sample on the incorrect side of the desicion boundary, the $y$ and the parentheses term will be opposite in sign, so $\gamma$ will be negative. Ideally, $\gamma$ for every sample in our training set should be positive. Let's define geometric margin as the minimum of all $\gamma$'s: 
$$
M = \mathop{\text{min}}_{i = 1,\dots,n} y_i (\frac{w}{||w||}\cdot x_i + \frac{b}{||w||})
$$
We want to maximize this geometric margin, which is the minimum geometric margin on all the individual training examples. 

